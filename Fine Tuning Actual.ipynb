{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb528b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02de9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33cb3a6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b683b1f6",
   "metadata": {},
   "source": [
    "This code will use the HuggingFace tutorial to fine-tune a model with a dataset. \n",
    "\n",
    "Tutorial: https://huggingface.co/course/chapter7/5?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c00502",
   "metadata": {},
   "source": [
    "Step 1: Prepare the corpus for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b91c40a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f9ccd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('PoetryFoundationData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92964c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Poem</th>\n",
       "      <th>Poet</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\r\\r\\n                    Objects Used to Prop...</td>\n",
       "      <td>\\r\\r\\nDog bone, stapler,\\r\\r\\ncribbage board, ...</td>\n",
       "      <td>Michelle Menting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\r\\r\\n                    The New Church\\r\\r\\n...</td>\n",
       "      <td>\\r\\r\\nThe old cupola glinted above the clouds,...</td>\n",
       "      <td>Lucia Cherciu</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\r\\r\\n                    Look for Me\\r\\r\\n   ...</td>\n",
       "      <td>\\r\\r\\nLook for me under the hood\\r\\r\\nof that ...</td>\n",
       "      <td>Ted Kooser</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\r\\r\\n                    Wild Life\\r\\r\\n     ...</td>\n",
       "      <td>\\r\\r\\nBehind the silo, the Mother Rabbit\\r\\r\\n...</td>\n",
       "      <td>Grace Cavalieri</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\\r\\r\\n                    Umbrella\\r\\r\\n      ...</td>\n",
       "      <td>\\r\\r\\nWhen I push your button\\r\\r\\nyou fly off...</td>\n",
       "      <td>Connie Wanek</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13849</th>\n",
       "      <td>13</td>\n",
       "      <td>\\r\\r\\n                    1-800-FEAR\\r\\r\\n    ...</td>\n",
       "      <td>\\r\\r\\nWe'd  like  to  talk  with  you  about  ...</td>\n",
       "      <td>Jody Gladding</td>\n",
       "      <td>Living,Social Commentaries,Popular Culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13850</th>\n",
       "      <td>14</td>\n",
       "      <td>\\r\\r\\n                    The Death of Atahual...</td>\n",
       "      <td>\\r\\r\\n\\r\\r\\n</td>\n",
       "      <td>William Jay Smith</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13851</th>\n",
       "      <td>15</td>\n",
       "      <td>\\r\\r\\n                    Poet's Wish\\r\\r\\n   ...</td>\n",
       "      <td>\\r\\r\\n\\r\\r\\n</td>\n",
       "      <td>William Jay Smith</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13852</th>\n",
       "      <td>0</td>\n",
       "      <td>\\r\\r\\n                    0\\r\\r\\n</td>\n",
       "      <td>\\r\\r\\n          Philosophic\\r\\r\\nin its comple...</td>\n",
       "      <td>Hailey Leithauser</td>\n",
       "      <td>Arts &amp; Sciences,Philosophy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13853</th>\n",
       "      <td>1</td>\n",
       "      <td>\\r\\r\\n                    !\\r\\r\\n</td>\n",
       "      <td>\\r\\r\\nDear Writers, I’m compiling the first in...</td>\n",
       "      <td>Wendy Videlock</td>\n",
       "      <td>Relationships,Gay, Lesbian, Queer,Arts &amp; Scien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13854 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              Title  \\\n",
       "0               0  \\r\\r\\n                    Objects Used to Prop...   \n",
       "1               1  \\r\\r\\n                    The New Church\\r\\r\\n...   \n",
       "2               2  \\r\\r\\n                    Look for Me\\r\\r\\n   ...   \n",
       "3               3  \\r\\r\\n                    Wild Life\\r\\r\\n     ...   \n",
       "4               4  \\r\\r\\n                    Umbrella\\r\\r\\n      ...   \n",
       "...           ...                                                ...   \n",
       "13849          13  \\r\\r\\n                    1-800-FEAR\\r\\r\\n    ...   \n",
       "13850          14  \\r\\r\\n                    The Death of Atahual...   \n",
       "13851          15  \\r\\r\\n                    Poet's Wish\\r\\r\\n   ...   \n",
       "13852           0  \\r\\r\\n                    0\\r\\r\\n                   \n",
       "13853           1  \\r\\r\\n                    !\\r\\r\\n                   \n",
       "\n",
       "                                                    Poem               Poet  \\\n",
       "0      \\r\\r\\nDog bone, stapler,\\r\\r\\ncribbage board, ...   Michelle Menting   \n",
       "1      \\r\\r\\nThe old cupola glinted above the clouds,...      Lucia Cherciu   \n",
       "2      \\r\\r\\nLook for me under the hood\\r\\r\\nof that ...         Ted Kooser   \n",
       "3      \\r\\r\\nBehind the silo, the Mother Rabbit\\r\\r\\n...    Grace Cavalieri   \n",
       "4      \\r\\r\\nWhen I push your button\\r\\r\\nyou fly off...       Connie Wanek   \n",
       "...                                                  ...                ...   \n",
       "13849  \\r\\r\\nWe'd  like  to  talk  with  you  about  ...      Jody Gladding   \n",
       "13850                                       \\r\\r\\n\\r\\r\\n  William Jay Smith   \n",
       "13851                                       \\r\\r\\n\\r\\r\\n  William Jay Smith   \n",
       "13852  \\r\\r\\n          Philosophic\\r\\r\\nin its comple...  Hailey Leithauser   \n",
       "13853  \\r\\r\\nDear Writers, I’m compiling the first in...     Wendy Videlock   \n",
       "\n",
       "                                                    Tags  \n",
       "0                                                    NaN  \n",
       "1                                                    NaN  \n",
       "2                                                    NaN  \n",
       "3                                                    NaN  \n",
       "4                                                    NaN  \n",
       "...                                                  ...  \n",
       "13849         Living,Social Commentaries,Popular Culture  \n",
       "13850                                                NaN  \n",
       "13851                                                NaN  \n",
       "13852                         Arts & Sciences,Philosophy  \n",
       "13853  Relationships,Gay, Lesbian, Queer,Arts & Scien...  \n",
       "\n",
       "[13854 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20a6c077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\r\\n                    Objects Used to Prop Open a Window\\r\\r\\n                '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16028e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Title', 'Poem']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c61218e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raunit/opt/anaconda3/envs/lign167/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/raunit/opt/anaconda3/envs/lign167/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df['Title'] = df['Title'].apply(lambda x: x.replace('\\r\\r\\n', ' ').strip())\n",
    "df['Poem'] = df['Poem'].apply(lambda x: x.replace('\\r\\r\\n', ' ').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03a1d06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Poem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Objects Used to Prop Open a Window</td>\n",
       "      <td>Dog bone, stapler, cribbage board, garlic pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The New Church</td>\n",
       "      <td>The old cupola glinted above the clouds, shone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Look for Me</td>\n",
       "      <td>Look for me under the hood of that old Chevrol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wild Life</td>\n",
       "      <td>Behind the silo, the Mother Rabbit hunches lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Umbrella</td>\n",
       "      <td>When I push your button you fly off the handle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13849</th>\n",
       "      <td>1-800-FEAR</td>\n",
       "      <td>We'd  like  to  talk  with  you  about  fear t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13850</th>\n",
       "      <td>The Death of Atahuallpa</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13851</th>\n",
       "      <td>Poet's Wish</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13852</th>\n",
       "      <td>0</td>\n",
       "      <td>Philosophic in its complex, ovoid emptiness, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13853</th>\n",
       "      <td>!</td>\n",
       "      <td>Dear Writers, I’m compiling the first in what ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13854 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Title  \\\n",
       "0      Objects Used to Prop Open a Window   \n",
       "1                          The New Church   \n",
       "2                             Look for Me   \n",
       "3                               Wild Life   \n",
       "4                                Umbrella   \n",
       "...                                   ...   \n",
       "13849                          1-800-FEAR   \n",
       "13850             The Death of Atahuallpa   \n",
       "13851                         Poet's Wish   \n",
       "13852                                   0   \n",
       "13853                                   !   \n",
       "\n",
       "                                                    Poem  \n",
       "0      Dog bone, stapler, cribbage board, garlic pres...  \n",
       "1      The old cupola glinted above the clouds, shone...  \n",
       "2      Look for me under the hood of that old Chevrol...  \n",
       "3      Behind the silo, the Mother Rabbit hunches lik...  \n",
       "4      When I push your button you fly off the handle...  \n",
       "...                                                  ...  \n",
       "13849  We'd  like  to  talk  with  you  about  fear t...  \n",
       "13850                                                     \n",
       "13851                                                     \n",
       "13852  Philosophic in its complex, ovoid emptiness, a...  \n",
       "13853  Dear Writers, I’m compiling the first in what ...  \n",
       "\n",
       "[13854 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4951a14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raunit/opt/anaconda3/envs/lign167/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/raunit/opt/anaconda3/envs/lign167/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df['Poem_len'] = df['Poem'].apply(lambda x: len(x))\n",
    "df['Title_len'] = df['Title'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae587b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all poems and titles which are too short or too long\n",
    "df = df[df['Poem_len'] > 0]\n",
    "df = df[df['Title_len'] > 0]\n",
    "df = df[df['Poem_len'] < 10000]\n",
    "df = df[df['Title_len'] < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "425711c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Poem</th>\n",
       "      <th>Poem_len</th>\n",
       "      <th>Title_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Objects Used to Prop Open a Window</td>\n",
       "      <td>Dog bone, stapler, cribbage board, garlic pres...</td>\n",
       "      <td>575</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The New Church</td>\n",
       "      <td>The old cupola glinted above the clouds, shone...</td>\n",
       "      <td>657</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Look for Me</td>\n",
       "      <td>Look for me under the hood of that old Chevrol...</td>\n",
       "      <td>389</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wild Life</td>\n",
       "      <td>Behind the silo, the Mother Rabbit hunches lik...</td>\n",
       "      <td>911</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Umbrella</td>\n",
       "      <td>When I push your button you fly off the handle...</td>\n",
       "      <td>629</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13835</th>\n",
       "      <td>!</td>\n",
       "      <td>Dear Writers, I’m compiling the first in what ...</td>\n",
       "      <td>211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13848</th>\n",
       "      <td>1 January 1965</td>\n",
       "      <td>The Wise Men will unlearn your name. Above you...</td>\n",
       "      <td>785</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13849</th>\n",
       "      <td>1-800-FEAR</td>\n",
       "      <td>We'd  like  to  talk  with  you  about  fear t...</td>\n",
       "      <td>661</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13852</th>\n",
       "      <td>0</td>\n",
       "      <td>Philosophic in its complex, ovoid emptiness, a...</td>\n",
       "      <td>472</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13853</th>\n",
       "      <td>!</td>\n",
       "      <td>Dear Writers, I’m compiling the first in what ...</td>\n",
       "      <td>211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13577 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Title  \\\n",
       "0      Objects Used to Prop Open a Window   \n",
       "1                          The New Church   \n",
       "2                             Look for Me   \n",
       "3                               Wild Life   \n",
       "4                                Umbrella   \n",
       "...                                   ...   \n",
       "13835                                   !   \n",
       "13848                      1 January 1965   \n",
       "13849                          1-800-FEAR   \n",
       "13852                                   0   \n",
       "13853                                   !   \n",
       "\n",
       "                                                    Poem  Poem_len  Title_len  \n",
       "0      Dog bone, stapler, cribbage board, garlic pres...       575         34  \n",
       "1      The old cupola glinted above the clouds, shone...       657         14  \n",
       "2      Look for me under the hood of that old Chevrol...       389         11  \n",
       "3      Behind the silo, the Mother Rabbit hunches lik...       911          9  \n",
       "4      When I push your button you fly off the handle...       629          8  \n",
       "...                                                  ...       ...        ...  \n",
       "13835  Dear Writers, I’m compiling the first in what ...       211          1  \n",
       "13848  The Wise Men will unlearn your name. Above you...       785         14  \n",
       "13849  We'd  like  to  talk  with  you  about  fear t...       661         10  \n",
       "13852  Philosophic in its complex, ovoid emptiness, a...       472          1  \n",
       "13853  Dear Writers, I’m compiling the first in what ...       211          1  \n",
       "\n",
       "[13577 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a45902",
   "metadata": {},
   "source": [
    "We're going to start with a dataset of just 1000 poem/title pairs for testing purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "978120ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(1000)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f94cfee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Poem</th>\n",
       "      <th>Poem_len</th>\n",
       "      <th>Title_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Breeder’s Cup</td>\n",
       "      <td>I. TO THE FATESThey cannot keep the peaceor th...</td>\n",
       "      <td>1301</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Canto I</td>\n",
       "      <td>And then went down to the ship, Set keel to br...</td>\n",
       "      <td>3371</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vow</td>\n",
       "      <td>It will be windy for a while until it isn’t. T...</td>\n",
       "      <td>1140</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Between Assassinations</td>\n",
       "      <td>Old court. Old chain net hanging in frayed lin...</td>\n",
       "      <td>1978</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Last Hour</td>\n",
       "      <td>Lean and sane in the last hour of a long fast ...</td>\n",
       "      <td>1339</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Sketch of a Man on a Platform</td>\n",
       "      <td>Man of absolute physical equilibrium You stand...</td>\n",
       "      <td>1337</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>My First Best Friend</td>\n",
       "      <td>My first best friend is Awful Ann— she socked ...</td>\n",
       "      <td>492</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Negative Space</td>\n",
       "      <td>1. I was born on a Tuesday in April. I didn't ...</td>\n",
       "      <td>9311</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Cold Trail</td>\n",
       "      <td>The feeling of time derives from heat, an agit...</td>\n",
       "      <td>491</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Compound Hibernation</td>\n",
       "      <td>Those who glance about me who cease to see ins...</td>\n",
       "      <td>1402</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Title  \\\n",
       "0                The Breeder’s Cup   \n",
       "1                          Canto I   \n",
       "2                              Vow   \n",
       "3           Between Assassinations   \n",
       "4                    The Last Hour   \n",
       "..                             ...   \n",
       "995  Sketch of a Man on a Platform   \n",
       "996           My First Best Friend   \n",
       "997                 Negative Space   \n",
       "998                     Cold Trail   \n",
       "999           Compound Hibernation   \n",
       "\n",
       "                                                  Poem  Poem_len  Title_len  \n",
       "0    I. TO THE FATESThey cannot keep the peaceor th...      1301         17  \n",
       "1    And then went down to the ship, Set keel to br...      3371          7  \n",
       "2    It will be windy for a while until it isn’t. T...      1140          3  \n",
       "3    Old court. Old chain net hanging in frayed lin...      1978         22  \n",
       "4    Lean and sane in the last hour of a long fast ...      1339         13  \n",
       "..                                                 ...       ...        ...  \n",
       "995  Man of absolute physical equilibrium You stand...      1337         29  \n",
       "996  My first best friend is Awful Ann— she socked ...       492         20  \n",
       "997  1. I was born on a Tuesday in April. I didn't ...      9311         14  \n",
       "998  The feeling of time derives from heat, an agit...       491         10  \n",
       "999  Those who glance about me who cease to see ins...      1402         20  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ebdd1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce101bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df, split='validation')\n",
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d09472c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Title', 'Poem', 'Poem_len', 'Title_len'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Title', 'Poem', 'Poem_len', 'Title_len'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de250f18",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we choose a pre-trained model and preprocess our data. \n",
    "The model I'll use is facebook/bart-base.\n",
    "See paper for explanation and analysis of why I chose this model. \n",
    "\n",
    "Note: After trying this model for a while, I am going to try using a BartForConditionalGeneration Model instead of just a BartModel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "504cbae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration #, BartModel\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06b53d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10ecef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# \n",
    "# model_checkpoint = 'facebook/bart-base'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d508f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 713, 16, 10, 1296, 7, 192, 114, 52, 64, 19233, 2072, 12461, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the tokenizer\n",
    "inputs = tokenizer(\"This is a test to see if we can tokenize correctly.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7a1e947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'This',\n",
       " 'Ġis',\n",
       " 'Ġa',\n",
       " 'Ġtest',\n",
       " 'Ġto',\n",
       " 'Ġsee',\n",
       " 'Ġif',\n",
       " 'Ġwe',\n",
       " 'Ġcan',\n",
       " 'Ġtoken',\n",
       " 'ize',\n",
       " 'Ġcorrectly',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "045e904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the max tokens for titles and poems\n",
    "\n",
    "max_poem = df.iloc[df['Poem_len'].idxmax()]['Poem']\n",
    "max_title = df.iloc[df['Title_len'].idxmax()]['Title']\n",
    "\n",
    "max_poem_length = len(tokenizer.convert_ids_to_tokens(tokenizer(max_poem, \n",
    "                                                                max_length=1024, \n",
    "                                                                truncation=True).input_ids))\n",
    "max_title_length = len(tokenizer.convert_ids_to_tokens(tokenizer(max_title, \n",
    "                                                                max_length=1024, \n",
    "                                                                truncation=True).input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16890aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max poem tokens length: 1024\n",
      "max title tokens length: 26\n"
     ]
    }
   ],
   "source": [
    "print(\"max poem tokens length: \" + str(max_poem_length))\n",
    "print(\"max title tokens length: \" + str(max_title_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fda808fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(data):\n",
    "    \n",
    "    model_inputs = tokenizer(data[\"Poem\"], max_length = max_poem_length, truncation=True)\n",
    "    \n",
    "    # should the first param be noted at target_text\n",
    "    labels = tokenizer(data[\"Title\"], max_length = max_title_length, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"] #TODO: Check if this column should be \"labels\" or \"label\"\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fc9ec9",
   "metadata": {},
   "source": [
    "The above code is giving me errors => The model_inputs[labels] column name should be \"labels\" otherwise the tokenized_dataset won't be able to remove the right column to make the values not strings. \n",
    "However, when I input the pre-processed data, it tells me that the 'label' column is missing in the BartConfig.forward() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd5b5e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0856a30567408d934af80c24dd4bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5437a1fb19ff483783fe15fb9f6cb321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bd527d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the arguments for the DataTrainer building off a Sequence to Sequence base Trainer\n",
    "\n",
    "batch_size = 8\n",
    "num_train_epochs = 8\n",
    "\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
    "model_name = 'bart-base'\n",
    "\n",
    "# arguments\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-poems\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513aa5b6",
   "metadata": {},
   "source": [
    "We now create a metric to evaluate the training => for text title generation the right metric is \"Rouge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1bbfdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/raunit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup evaluation metric for training\n",
    "\n",
    "import evaluate\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # sentence tokenizer\n",
    "\n",
    "metric = evaluate.load(\"rouge\")\n",
    "nltk.download(\"punkt\") # we need to download this for some reason to run the metric.compute function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2f62367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.923076923076923,\n",
       " 'rouge2': 0.7272727272727272,\n",
       " 'rougeL': 0.923076923076923,\n",
       " 'rougeLsum': 0.923076923076923}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functions to test the rouge computational metric\n",
    "\n",
    "generated_title = \"I absolutely loved reading the Hunger Games\"\n",
    "reference_title = \"I loved reading the Hunger Games\"\n",
    "\n",
    "scores = metric.compute(predictions=[generated_title], references=[reference_title])\n",
    "\n",
    "scores # this returns only the fmeasure (nothing else though I'm not sure why...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d887d0",
   "metadata": {},
   "source": [
    "We interperet the above rouge scores like this:\n",
    "- rouge 1 is the ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a67e2e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 7.88, 'rouge2': 3.79, 'rougeL': 7.54, 'rougeLsum': 7.56}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:1])\n",
    "\n",
    "def evaluate_baseline(dataset, metric):\n",
    "    summaries = [one_sentence_summary(text) for text in dataset[\"Poem\"]]\n",
    "    return metric.compute(predictions=summaries, references=dataset[\"Title\"])\n",
    "\n",
    "score = evaluate_baseline(dataset[\"train\"], metric)\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "rouge_dict = dict((rn, round(score[rn] * 100, 2)) for rn in rouge_names)\n",
    "rouge_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411dd037",
   "metadata": {},
   "source": [
    "^ we interepret these as such:\n",
    "- Firstly, the rouge2 score is much lower... (here's why: ??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1752634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function offically computes the metrics of the predictions so we can calculate during the training\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    \n",
    "    # Decode generated titles into text\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Decode reference titles into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    # Extract the median scores\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfe26139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the data collator to pad the inputs and outputs\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "daa3987a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0, 5625,   18,  ...,    1,    1,    1],\n",
       "        [   0,  100,   91,  ..., 1840,    4,    2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[    0, 44728,  1208,  7239,     2],\n",
       "        [    0,   771,  3239, 12521,     2]]), 'decoder_input_ids': tensor([[    2,     0, 44728,  1208,  7239],\n",
       "        [    2,     0,   771,  3239, 12521]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the data collator\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(dataset[\"train\"].column_names)\n",
    "features = [tokenized_datasets[\"train\"][i] for i in range(2)]\n",
    "data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33903954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe the train_dataset\n",
    "\n",
    "tokenized_datasets['train'].features['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16c931ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/rkbulk/bart-base-finetuned-poems into local empty directory.\n",
      "WARNING:huggingface_hub.repository:Cloning https://huggingface.co/rkbulk/bart-base-finetuned-poems into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba752bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raunit/opt/anaconda3/envs/lign167/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 800\n",
      "  Number of trainable parameters = 139420416\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c101612443461696814d3d6d47c600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7974, 'learning_rate': 4.9e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d35c907ecce475ea9bfe97aa7ced574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1509082317352295, 'eval_rouge1': 17.5339, 'eval_rouge2': 8.1625, 'eval_rougeL': 17.2189, 'eval_rougeLsum': 17.2855, 'eval_runtime': 500.4174, 'eval_samples_per_second': 0.4, 'eval_steps_per_second': 0.05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.721, 'learning_rate': 4.2e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6956efae8f4d3b853279bb0580dd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1969778537750244, 'eval_rouge1': 16.9107, 'eval_rouge2': 8.1464, 'eval_rougeL': 16.5554, 'eval_rougeLsum': 16.7396, 'eval_runtime': 487.5616, 'eval_samples_per_second': 0.41, 'eval_steps_per_second': 0.051, 'epoch': 2.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/92/g2k2dd6106d0x3q_zf2j28280000gp/T/ipykernel_41692/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/lign167/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         )\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/lign167/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m                 if (\n",
      "\u001b[0;32m~/opt/anaconda3/envs/lign167/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2526\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/lign167/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/lign167/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46392fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16b79c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bart-base-finetuned-poems\n",
      "Configuration saved in bart-base-finetuned-poems/config.json\n",
      "Model weights saved in bart-base-finetuned-poems/pytorch_model.bin\n",
      "tokenizer config file saved in bart-base-finetuned-poems/tokenizer_config.json\n",
      "Special tokens file saved in bart-base-finetuned-poems/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf04012620ba4b90ac94db1118405679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/532M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30407a492b4949ec98ab392e201033cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin: 100%|##########| 3.42k/3.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/rkbulk/bart-base-finetuned-poems\n",
      "   a263817..4ddd8a8  main -> main\n",
      "\n",
      "WARNING:huggingface_hub.repository:remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/rkbulk/bart-base-finetuned-poems\n",
      "   a263817..4ddd8a8  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}}\n",
      "To https://huggingface.co/rkbulk/bart-base-finetuned-poems\n",
      "   4ddd8a8..4e0ffa7  main -> main\n",
      "\n",
      "WARNING:huggingface_hub.repository:To https://huggingface.co/rkbulk/bart-base-finetuned-poems\n",
      "   4ddd8a8..4e0ffa7  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/rkbulk/bart-base-finetuned-poems/commit/4ddd8a85248c8a581a5e60d8f4f1a3c85b91d916'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\", tags=\"summarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a740a5",
   "metadata": {},
   "source": [
    "Now that we've fine-tuned our model, let's use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce4a7b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45bb29140614804963e120aeb350d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/raunit/.cache/huggingface/hub/models--rkbulk--bart-base-finetuned-poems/snapshots/4e0ffa7b15c7ccae4a4b101e76a8d047cff0253f/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"rkbulk/bart-base-finetuned-poems\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/raunit/.cache/huggingface/hub/models--rkbulk--bart-base-finetuned-poems/snapshots/4e0ffa7b15c7ccae4a4b101e76a8d047cff0253f/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"rkbulk/bart-base-finetuned-poems\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c35f19614e4835a1d52d44a9a6bf84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /Users/raunit/.cache/huggingface/hub/models--rkbulk--bart-base-finetuned-poems/snapshots/4e0ffa7b15c7ccae4a4b101e76a8d047cff0253f/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at rkbulk/bart-base-finetuned-poems.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62301f9c60b4005bccfa9f4d22bb033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb686309c6e24279ada4edee546eac58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/999k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8f0d7b42434bcc9875bdb06813dcde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a67bdf39f84710b5560fa0853af3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /Users/raunit/.cache/huggingface/hub/models--rkbulk--bart-base-finetuned-poems/snapshots/4e0ffa7b15c7ccae4a4b101e76a8d047cff0253f/vocab.json\n",
      "loading file merges.txt from cache at /Users/raunit/.cache/huggingface/hub/models--rkbulk--bart-base-finetuned-poems/snapshots/4e0ffa7b15c7ccae4a4b101e76a8d047cff0253f/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/raunit/.cache/huggingface/hub/models--rkbulk--bart-base-finetuned-poems/snapshots/4e0ffa7b15c7ccae4a4b101e76a8d047cff0253f/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/raunit/.cache/huggingface/hub/models--rkbulk--bart-base-finetuned-poems/snapshots/4e0ffa7b15c7ccae4a4b101e76a8d047cff0253f/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "hub_model_id = \"rkbulk/bart-base-finetuned-poems\"\n",
    "summarizer = pipeline(\"summarization\", model=hub_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31be452",
   "metadata": {},
   "source": [
    "Testing the model here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c84a3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it's your 1st year of college & you should be missing home by now but mostly you don't. you read the Chicago newspapers & call family on Sundays. you pick up going to church at a place adjacent to the projects. you're not from the projects & the ones in Chicago seem worse but there's comfort in being around plainspoken folk. the church folk feed you & also cook you food. you take African American studies classes & sleep through Spanish & write poems at night. you read the newspaper. you consider pledging a fraternity. you go to parties to watch people. you don't miss home. you call your ex girl a lot. you imagine her face across the phone line. you stare at the scar on her chin. it is shiny & smooth. you read the newspaper. you text new girls mostly. you invite them to play cards & bet clothes or take them to dinner on your birthday so you don't spend it alone or you share their extra-long twin beds or you just text them. it's your 1st year of college & your nephew is tiny & your niece is young enough to be happy & the world is new & you are not going home for Thanksgiving. you are in the South at a new friend's house. you go to church with his family & to his old high school's basketball game & to his malls & to his grandmother's house. you did not make your team past 9th grade & never went to malls much. your grandmother had been dead for 2 years now. you read the newspaper. his family are nice people. you do not miss home. you go back to school. you stop talking to your ex girl. she has a new guy. you do not miss home. you write poems. you read the newspaper. there are still more kids dying. your 1st year of college & you should be missing but you're still here. you write papers about black people & voting & violence & families & that is the same paper. you don't read the newspaper. you have finals to finish. you go to church on Sunday with your new friend & you talk to new girls & consider pledging. you have heard the fraternities will haze you. you have heard about beating but you are not from the projects & you are not in Chicago. you stop reading the newspaper. you decide to kiss a girl & mean it. you decide to pledge a fraternity. you should have more information about the newspaper. & the girl. & the fraternity. you should call home more. you don't  read the newspapers or call. you are not from the projects or Chicago. you do not miss home. or your ex girl. or your newspaper. there are still more kids dying. you convince your new friend to pledge the fraternity. he worries about the hazing, the beatings. you tell him this is an opportunity. don't miss it.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[754]['Poem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d198213c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"it's your 1st year of college & you should be missing home by now\"}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(df.iloc[754]['Poem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bce465fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'recycling'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[754]['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c02b5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('lign167')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "841be618abf1684b3a857fe578511fd376c89368ab3267e2ee74e030dadd7b52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
