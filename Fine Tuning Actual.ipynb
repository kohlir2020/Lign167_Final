{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb528b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb3a6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b683b1f6",
   "metadata": {},
   "source": [
    "This code will use the HuggingFace tutorial to fine-tune a model with a dataset. \n",
    "\n",
    "Tutorial: https://huggingface.co/course/chapter7/5?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c00502",
   "metadata": {},
   "source": [
    "Step 1: Prepare the corpus for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c40a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ccd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('PoetryFoundationData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92964c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16028e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Title', 'Poem']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c61218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title'] = df['Title'].apply(lambda x: x.replace('\\r\\r\\n', ' ').strip())\n",
    "df['Poem'] = df['Poem'].apply(lambda x: x.replace('\\r\\r\\n', ' ').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a1d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4951a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Poem_len'] = df['Poem'].apply(lambda x: len(x))\n",
    "df['Title_len'] = df['Title'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae587b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all poems and titles which are too short or too long\n",
    "df = df[df['Poem_len'] > 0]\n",
    "df = df[df['Title_len'] > 0]\n",
    "df = df[df['Poem_len'] < 10000]\n",
    "df = df[df['Title_len'] < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425711c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a45902",
   "metadata": {},
   "source": [
    "We're going to start with a dataset of just 1000 poem/title pairs for testing purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978120ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(1000)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f94cfee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebdd1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce101bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df, split='validation')\n",
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d09472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de250f18",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we choose a pre-trained model and preprocess our data. \n",
    "The model I'll use is facebook/bart-base.\n",
    "See paper for explanation and analysis of why I chose this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504cbae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartModel\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = BartModel.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b53d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ecef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# \n",
    "# model_checkpoint = 'facebook/bart-base'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d508f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the tokenizer\n",
    "inputs = tokenizer(\"This is a test to see if we can tokenize correctly.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the max tokens for titles and poems\n",
    "\n",
    "max_poem = df.iloc[df['Poem_len'].idxmax()]['Poem']\n",
    "max_title = df.iloc[df['Title_len'].idxmax()]['Title']\n",
    "\n",
    "max_poem_length = len(tokenizer.convert_ids_to_tokens(tokenizer(max_poem, \n",
    "                                                                max_length=1024, \n",
    "                                                                truncation=True).input_ids))\n",
    "max_title_length = len(tokenizer.convert_ids_to_tokens(tokenizer(max_title, \n",
    "                                                                max_length=1024, \n",
    "                                                                truncation=True).input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16890aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max poem tokens length: \" + str(max_poem_length))\n",
    "print(\"max title tokens length: \" + str(max_title_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda808fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(data):\n",
    "    \n",
    "    model_inputs = tokenizer(data[\"Poem\"], max_length = max_poem_length, truncation=True)\n",
    "    \n",
    "    # should the first param be noted at target_text\n",
    "    labels = tokenizer(data[\"Title\"], max_length = max_title_length, truncation=True)\n",
    "    \n",
    "    model_inputs[\"label\"] = labels[\"input_ids\"] #TODO: Check if this column should be \"labels\" or \"label\"\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5b5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd527d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the arguments for the DataTrainer building off a Sequence to Sequence base Trainer\n",
    "\n",
    "batch_size = 8\n",
    "num_train_epochs = 8\n",
    "\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
    "model_name = 'bart-base'\n",
    "\n",
    "# arguments\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-poems\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513aa5b6",
   "metadata": {},
   "source": [
    "We now create a metric to evaluate the training => for text title generation the right metric is \"Rouge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bbfdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup evaluation metric for training\n",
    "\n",
    "import evaluate\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # sentence tokenizer\n",
    "\n",
    "metric = evaluate.load(\"rouge\")\n",
    "nltk.download(\"punkt\") # we need to download this for some reason to run the metric.compute function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f62367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to test the rouge computational metric\n",
    "\n",
    "generated_title = \"I absolutely loved reading the Hunger Games\"\n",
    "reference_title = \"I loved reading the Hunger Games\"\n",
    "\n",
    "scores = metric.compute(predictions=[generated_title], references=[reference_title])\n",
    "\n",
    "scores # this returns only the fmeasure (nothing else though I'm not sure why...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d887d0",
   "metadata": {},
   "source": [
    "We interperet the above rouge scores like this:\n",
    "- rouge 1 is the ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67e2e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:1])\n",
    "\n",
    "def evaluate_baseline(dataset, metric):\n",
    "    summaries = [one_sentence_summary(text) for text in dataset[\"Poem\"]]\n",
    "    return metric.compute(predictions=summaries, references=dataset[\"Title\"])\n",
    "\n",
    "score = evaluate_baseline(dataset[\"train\"], metric)\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "rouge_dict = dict((rn, round(score[rn] * 100, 2)) for rn in rouge_names)\n",
    "rouge_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411dd037",
   "metadata": {},
   "source": [
    "^ we interepret these as such:\n",
    "- Firstly, the rouge2 score is much lower... (here's why: ??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1752634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function offically computes the metrics of the predictions so we can calculate during the training\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    \n",
    "    # Decode generated titles into text\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Decode reference titles into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    # Extract the median scores\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe26139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the data collator to pad the inputs and outputs\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa3987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the data collator\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(dataset[\"train\"].column_names)\n",
    "features = [tokenized_datasets[\"train\"][i] for i in range(2)]\n",
    "data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33903954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe the train_dataset\n",
    "\n",
    "tokenized_datasets['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c931ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba752bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46392fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b79c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\", tags=\"summarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a740a5",
   "metadata": {},
   "source": [
    "Now that we've fine-tuned our model, let's use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a7b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "hub_model_id = \"huggingface-course/bart-base-finetuned-poems\"\n",
    "summarizer = pipeline(\"summarization\", model=hub_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer('POEM this is a test poem..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31be452",
   "metadata": {},
   "source": [
    "Testing the model here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce465fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('lign167')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "841be618abf1684b3a857fe578511fd376c89368ab3267e2ee74e030dadd7b52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
